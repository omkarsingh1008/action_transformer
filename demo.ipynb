{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-02 10:05:23.221345: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-02 10:05:23.221385: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/ubuntu/anaconda3/envs/act/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ubuntu/anaconda3/envs/act/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.9.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from utils.tools import read_yaml\n",
    "# GENERAL LIBRARIES \n",
    "import math\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "# MACHINE LEARNING LIBRARIES\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "# OPTUNA\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from utils.transformer import TransformerEncoder, PatchClassEmbedding, Patches\n",
    "from utils.data import load_mpose, random_flip, random_noise, one_hot\n",
    "from utils.tools import CustomSchedule, CosineSchedule\n",
    "from utils.tools import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GPU': 0, 'VERBOSE': True, 'MODEL_DIR': 'bin/', 'RESULTS_DIR': 'results/', 'LOG_DIR': 'logs/', 'MODEL_NAME': 'AcT', 'SPLITS': 3, 'FOLDS': 10, 'SEEDS': [11331], 'VAL_SIZE': 0.1, 'FRAMES': 30, 'CHANNELS': 4, 'DATASET': 'openpose', 'openpose': {'KEYPOINTS': 13}, 'posenet': {'KEYPOINTS': 17}, 'CLASSES': 20, 'LABELS': ['standing', 'check-watch', 'cross-arms', 'scratch-head', 'sit-down', 'get-up', 'turn-around', 'walking', 'wave1', 'boxing', 'kicking', 'pointing', 'pick-up', 'bending', 'hands-clapping', 'wave2', 'jogging', 'jumping', 'pjump', 'running'], 'FLIP_P': 0.5, 'RN_STD': 0.03, 'PATCH_SIZE': 1, 'MODEL_SIZE': 'micro', 'micro': {'N_HEADS': 1, 'N_LAYERS': 4, 'EMBED_DIM': 64, 'DROPOUT': 0.3, 'MLP': 256}, 'small': {'N_HEADS': 2, 'N_LAYERS': 5, 'EMBED_DIM': 128, 'DROPOUT': 0.3, 'MLP': 256}, 'base': {'N_HEADS': 3, 'N_LAYERS': 6, 'EMBED_DIM': 192, 'DROPOUT': 0.3, 'MLP': 256}, 'large': {'N_HEADS': 4, 'N_LAYERS': 6, 'EMBED_DIM': 256, 'DROPOUT': 0.4, 'MLP': 512}, 'N_EPOCHS': 1, 'BATCH_SIZE': 512, 'WEIGHT_DECAY': 0.0001, 'WARMUP_PERC': 0.3, 'STEP_PERC': 0.8, 'N_TRIALS': 256}\n"
     ]
    }
   ],
   "source": [
    "config = read_yaml('utils/config.yaml')\n",
    "\n",
    "print(config)\n",
    "model_size = config['MODEL_SIZE']\n",
    "n_heads = config[model_size]['N_HEADS']\n",
    "n_layers = config[model_size]['N_LAYERS']\n",
    "embed_dim = config[model_size]['EMBED_DIM']\n",
    "dropout = config[model_size]['DROPOUT']\n",
    "mlp_head_size = config[model_size]['MLP']\n",
    "activation = tf.nn.gelu\n",
    "d_model = 64 * n_heads\n",
    "d_ff = d_model * 4\n",
    "\n",
    "split = 1\n",
    "fold = 0\n",
    "trial = None\n",
    "bin_path = config['MODEL_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    X_train, y_train, X_test, y_test = load_mpose(config['DATASET'], split, verbose=False)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                        test_size=config['VAL_SIZE'],\n",
    "                                                        random_state=42,\n",
    "                                                        stratify=y_train)\n",
    "            \n",
    "    ds_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    ds_train = ds_train.map(lambda x,y : one_hot(x,y,config['CLASSES']), \n",
    "                            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds_train = ds_train.cache()\n",
    "    ds_train = ds_train.map(random_flip, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds_train = ds_train.map(random_noise, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds_train = ds_train.shuffle(X_train.shape[0])\n",
    "    ds_train = ds_train.batch(config['BATCH_SIZE'])\n",
    "    ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    ds_val = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "    ds_val = ds_val.map(lambda x,y : one_hot(x,y,config['CLASSES']), \n",
    "                            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds_val = ds_val.cache()\n",
    "    ds_val = ds_val.batch(config['BATCH_SIZE'])\n",
    "    ds_val = ds_val.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    ds_test = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    ds_test = ds_test.map(lambda x,y : one_hot(x,y,config['CLASSES']), \n",
    "                            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds_test = ds_test.cache()\n",
    "    ds_test = ds_test.batch(config['BATCH_SIZE'])\n",
    "    ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds_train, ds_test,ds_val,X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-02 10:05:59.204304: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-02 10:05:59.204337: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-02 10:05:59.204360: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu-ThinkPad-X250): /proc/driver/nvidia/version does not exist\n",
      "2022-08-02 10:05:59.206839: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-02 10:05:59.212089: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 141086400 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "ds_train, ds_test,ds_val,x_train = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = tuple(zip(*ds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_demo=X[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 52)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(tf.nn.softmax(model.predict(tf.concat(X, axis=0)), axis=-1),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 30, 52), dtype=float64, numpy=\n",
       "array([[[ 0.00448911, -0.36499393,  0.00448911, ...,  2.36464603,\n",
       "          0.09099247,  2.36464603],\n",
       "        [ 0.02615695, -0.36490139,  0.02166783, ...,  2.39078909,\n",
       "          0.02501499,  0.02614306],\n",
       "        [ 0.03384959, -0.37170456,  0.00769264, ...,  2.41642411,\n",
       "          0.01352134,  0.02563502],\n",
       "        ...,\n",
       "        [ 0.03451963, -0.36856423,  0.01087998, ...,  2.29394845,\n",
       "          0.01014142, -0.02388365],\n",
       "        [ 0.02348415, -0.40197094, -0.01103548, ...,  2.32914274,\n",
       "         -0.00631095,  0.03519429],\n",
       "        [ 0.03274832, -0.40224001,  0.00926417, ...,  2.32244785,\n",
       "          0.00881321, -0.00669489]]])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(x_train[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 30, 52)]          0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 30, 64)            3392      \n",
      "                                                                 \n",
      " patch_class_embedding (Patc  (None, 31, 64)           2048      \n",
      " hClassEmbedding)                                                \n",
      "                                                                 \n",
      " transformer_encoder (Transf  (None, 31, 64)           199936    \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 64)                0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               16640     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 20)                5140      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 227,156\n",
      "Trainable params: 227,156\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def build_act(transformer):\n",
    "        inputs = tf.keras.layers.Input(shape=(config['FRAMES'], \n",
    "                                              config[config['DATASET']]['KEYPOINTS']*config['CHANNELS']))\n",
    "        x = tf.keras.layers.Dense(d_model)(inputs)\n",
    "        x = PatchClassEmbedding(d_model, config['FRAMES'])(x)\n",
    "        x = transformer(x)\n",
    "        x = tf.keras.layers.Lambda(lambda x: x[:,0,:])(x)\n",
    "        x = tf.keras.layers.Dense(mlp_head_size)(x)\n",
    "        outputs = tf.keras.layers.Dense(config['CLASSES'])(x)\n",
    "        return tf.keras.models.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "transformer = TransformerEncoder(d_model, n_heads,d_ff, dropout, activation, n_layers)\n",
    "model = build_act(transformer)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#lr = CustomSchedule(d_model, \n",
    "#             warmup_steps=len(ds_train)*config['N_EPOCHS']*config['WARMUP_PERC'],\n",
    "#             decay_step=len(ds_train)*config['N_EPOCHS']*config['STEP_PERC'])\n",
    "\n",
    "optimizer = tfa.optimizers.AdamW(weight_decay=config['WEIGHT_DECAY'])\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "                           loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "                           metrics=[tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")])\n",
    "\n",
    "\n",
    "model.load_weights(\"bin/AcT_micro_3_9.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(tf.nn.softmax(model.predict(tf.concat(X, axis=0)), axis=-1),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    }
   ],
   "source": [
    "y_demo=model.predict(tf.expand_dims(x_train[0], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_demo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(tf.nn.softmax(y_demo,axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpose import MPOSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MPOSE2021 with openpose Pose Extractor\n",
      "Downloading Data...\n",
      "File exists in /home/ubuntu/.mpose/openpose.zip. specify overwrite=True if intended\n",
      "Extracting Data...\n",
      "File exists in /home/ubuntu/.mpose/openpose/. specify overwrite=True if intended\n"
     ]
    }
   ],
   "source": [
    "dataset = MPOSE(pose_extractor=\"openpose\", \n",
    "                split=split, \n",
    "                preprocess=None, \n",
    "                velocities=True, \n",
    "                remove_zip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=dataset.add_velocities(overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Dataset Information----\n",
      "Pose Extractor: openpose\n",
      "Split: 1\n",
      "X_train shape: (12562, 30, 25, 5)\n",
      "X_test shape: (2867, 30, 25, 5)\n",
      "Min-Max feature ranges:\n",
      "x: [0.0, 828.042]\n",
      "y: [0.0, 558.729]\n",
      "Vx: [-803.5, 790.405]\n",
      "Vy: [-511.387, 555.379]\n",
      "p: [0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "dataset.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.remove_velocities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"/home/ubuntu/.mpose/openpose/1\" + '/X_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12562, 30, 25, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_velocities(X_train,overwrite=False):\n",
    " \n",
    "        \n",
    "        seq_list = []\n",
    "        for seq in X_train:\n",
    "          \n",
    "            v1 = np.zeros((30+1, seq.shape[1], 3-1))\n",
    "            v2 = np.zeros((30+1, seq.shape[1], 3-1))\n",
    "            \n",
    "            v1[1:,...] = seq[:,:,:2]\n",
    "            v2[:30,...] = seq[:,:,:2]\n",
    "            vel = (v2-v1)[:-1,...]\n",
    "            data = np.concatenate((seq[:,:,:2], vel), axis=-1)\n",
    "            data = np.concatenate((data, seq[:,:,-1:]), axis=-1)       \n",
    "            seq_list.append(data)\n",
    "        X_train = np.stack(seq_list)\n",
    "\n",
    "        return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=add_velocities(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12562, 30, 25, 5)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12562, 30, 25, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00],\n",
       "       [2.34870e+02, 6.64126e+01, 2.34870e+02, 6.64126e+01, 8.40771e-01],\n",
       "       [2.49527e+02, 6.71612e+01, 2.49527e+02, 6.71612e+01, 8.75739e-01],\n",
       "       [2.54645e+02, 1.00110e+02, 2.54645e+02, 1.00110e+02, 8.40024e-01],\n",
       "       [2.60504e+02, 1.25009e+02, 2.60504e+02, 1.25009e+02, 8.68385e-01],\n",
       "       [2.19502e+02, 6.49461e+01, 2.19502e+02, 6.49461e+01, 8.49073e-01],\n",
       "       [2.18773e+02, 9.57001e+01, 2.18773e+02, 9.57001e+01, 7.49432e-01],\n",
       "       [2.26101e+02, 1.09644e+02, 2.26101e+02, 1.09644e+02, 2.06299e-01],\n",
       "       [2.34149e+02, 1.19892e+02, 2.34149e+02, 1.19892e+02, 6.39379e-01],\n",
       "       [2.43674e+02, 1.20619e+02, 2.43674e+02, 1.20619e+02, 6.84198e-01],\n",
       "       [2.45144e+02, 1.60173e+02, 2.45144e+02, 1.60173e+02, 7.38917e-01],\n",
       "       [2.45136e+02, 2.00453e+02, 2.45136e+02, 2.00453e+02, 7.67821e-01],\n",
       "       [2.25329e+02, 1.19862e+02, 2.25329e+02, 1.19862e+02, 6.53879e-01],\n",
       "       [2.25361e+02, 1.55790e+02, 2.25361e+02, 1.55790e+02, 8.08271e-01],\n",
       "       [2.20954e+02, 1.90916e+02, 2.20954e+02, 1.90916e+02, 8.25551e-01],\n",
       "       [2.49514e+02, 4.29509e+01, 2.49514e+02, 4.29509e+01, 7.84337e-02],\n",
       "       [0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00],\n",
       "       [2.46597e+02, 4.37383e+01, 2.46597e+02, 4.37383e+01, 8.46595e-01],\n",
       "       [2.31951e+02, 4.37016e+01, 2.31951e+02, 4.37016e+01, 3.80911e-01],\n",
       "       [2.31972e+02, 1.90195e+02, 2.31972e+02, 1.90195e+02, 6.86278e-01],\n",
       "       [2.31209e+02, 1.89464e+02, 2.31209e+02, 1.89464e+02, 6.13726e-01],\n",
       "       [2.19474e+02, 1.96053e+02, 2.19474e+02, 1.96053e+02, 8.02246e-01],\n",
       "       [2.62695e+02, 2.01890e+02, 2.62695e+02, 2.01890e+02, 7.14654e-01],\n",
       "       [2.60492e+02, 2.03379e+02, 2.60492e+02, 2.03379e+02, 7.28853e-01],\n",
       "       [2.42908e+02, 2.04856e+02, 2.42908e+02, 2.04856e+02, 7.49616e-01]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('act': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43bd45374c4c1900a68ba5a1e51ad14b9d0c49e5cdd2c40380f8da0762609752"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
