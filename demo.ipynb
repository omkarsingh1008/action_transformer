{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omkar/anaconda3/envs/act/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/omkar/anaconda3/envs/act/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.9.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from utils.tools import read_yaml\n",
    "# GENERAL LIBRARIES \n",
    "import math\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "# MACHINE LEARNING LIBRARIES\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "# OPTUNA\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from utils.transformer import TransformerEncoder, PatchClassEmbedding, Patches\n",
    "from utils.data import load_mpose, random_flip, random_noise, one_hot\n",
    "from utils.tools import CustomSchedule, CosineSchedule\n",
    "from utils.tools import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GPU': 0, 'VERBOSE': True, 'MODEL_DIR': 'bin/', 'RESULTS_DIR': 'results/', 'LOG_DIR': 'logs/', 'MODEL_NAME': 'AcT', 'SPLITS': 3, 'FOLDS': 10, 'SEEDS': [11331], 'VAL_SIZE': 0.1, 'FRAMES': 30, 'CHANNELS': 4, 'DATASET': 'openpose', 'openpose': {'KEYPOINTS': 13}, 'posenet': {'KEYPOINTS': 17}, 'CLASSES': 20, 'LABELS': ['standing', 'check-watch', 'cross-arms', 'scratch-head', 'sit-down', 'get-up', 'turn-around', 'walking', 'wave1', 'boxing', 'kicking', 'pointing', 'pick-up', 'bending', 'hands-clapping', 'wave2', 'jogging', 'jumping', 'pjump', 'running'], 'FLIP_P': 0.5, 'RN_STD': 0.03, 'PATCH_SIZE': 1, 'MODEL_SIZE': 'micro', 'micro': {'N_HEADS': 1, 'N_LAYERS': 4, 'EMBED_DIM': 64, 'DROPOUT': 0.3, 'MLP': 256}, 'small': {'N_HEADS': 2, 'N_LAYERS': 5, 'EMBED_DIM': 128, 'DROPOUT': 0.3, 'MLP': 256}, 'base': {'N_HEADS': 3, 'N_LAYERS': 6, 'EMBED_DIM': 192, 'DROPOUT': 0.3, 'MLP': 256}, 'large': {'N_HEADS': 4, 'N_LAYERS': 6, 'EMBED_DIM': 256, 'DROPOUT': 0.4, 'MLP': 512}, 'N_EPOCHS': 1, 'BATCH_SIZE': 512, 'WEIGHT_DECAY': 0.0001, 'WARMUP_PERC': 0.3, 'STEP_PERC': 0.8, 'N_TRIALS': 256}\n"
     ]
    }
   ],
   "source": [
    "config = read_yaml('utils/config.yaml')\n",
    "\n",
    "print(config)\n",
    "model_size = config['MODEL_SIZE']\n",
    "n_heads = config[model_size]['N_HEADS']\n",
    "n_layers = config[model_size]['N_LAYERS']\n",
    "embed_dim = config[model_size]['EMBED_DIM']\n",
    "dropout = config[model_size]['DROPOUT']\n",
    "mlp_head_size = config[model_size]['MLP']\n",
    "activation = tf.nn.gelu\n",
    "d_model = 64 * n_heads\n",
    "d_ff = d_model * 4\n",
    "\n",
    "split = 1\n",
    "fold = 0\n",
    "trial = None\n",
    "bin_path = config['MODEL_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    X_train, y_train, X_test, y_test = load_mpose(config['DATASET'], split, verbose=False)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                        test_size=config['VAL_SIZE'],\n",
    "                                                        random_state=42,\n",
    "                                                        stratify=y_train)\n",
    "            \n",
    "    ds_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    ds_train = ds_train.map(lambda x,y : one_hot(x,y,config['CLASSES']), \n",
    "                            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds_train = ds_train.cache()\n",
    "    ds_train = ds_train.map(random_flip, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds_train = ds_train.map(random_noise, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds_train = ds_train.shuffle(X_train.shape[0])\n",
    "    ds_train = ds_train.batch(config['BATCH_SIZE'])\n",
    "    ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    ds_val = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "    ds_val = ds_val.map(lambda x,y : one_hot(x,y,config['CLASSES']), \n",
    "                            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds_val = ds_val.cache()\n",
    "    ds_val = ds_val.batch(config['BATCH_SIZE'])\n",
    "    ds_val = ds_val.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    ds_test = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    ds_test = ds_test.map(lambda x,y : one_hot(x,y,config['CLASSES']), \n",
    "                            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds_test = ds_test.cache()\n",
    "    ds_test = ds_test.batch(config['BATCH_SIZE'])\n",
    "    ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds_train, ds_test,ds_val,X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-03 10:38:13.193919: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-03 10:38:13.193964: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-03 10:38:13.193997: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu-ThinkPad-X250): /proc/driver/nvidia/version does not exist\n",
      "2022-08-03 10:38:13.194409: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-03 10:38:13.195780: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 141086400 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "ds_train, ds_test,ds_val,x_train = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = tuple(zip(*ds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512, 30, 52])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_demo=X[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11305, 30, 52)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(tf.nn.softmax(model.predict(tf.concat(X, axis=0)), axis=-1),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 30, 52), dtype=float64, numpy=\n",
       "array([[[ 0.00448911, -0.36499393,  0.00448911, ...,  2.36464603,\n",
       "          0.09099247,  2.36464603],\n",
       "        [ 0.02615695, -0.36490139,  0.02166783, ...,  2.39078909,\n",
       "          0.02501499,  0.02614306],\n",
       "        [ 0.03384959, -0.37170456,  0.00769264, ...,  2.41642411,\n",
       "          0.01352134,  0.02563502],\n",
       "        ...,\n",
       "        [ 0.03451963, -0.36856423,  0.01087998, ...,  2.29394845,\n",
       "          0.01014142, -0.02388365],\n",
       "        [ 0.02348415, -0.40197094, -0.01103548, ...,  2.32914274,\n",
       "         -0.00631095,  0.03519429],\n",
       "        [ 0.03274832, -0.40224001,  0.00926417, ...,  2.32244785,\n",
       "          0.00881321, -0.00669489]]])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(x_train[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-03 17:19:03.469993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-03 17:19:03.549246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-03 17:19:03.550972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-03 17:19:03.560133: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-03 17:19:03.562975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-03 17:19:03.564730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-03 17:19:03.566562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-03 17:19:06.148488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-03 17:19:06.153423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-03 17:19:06.154939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-03 17:19:06.156304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2866 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 30, 52)]          0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 30, 64)            3392      \n",
      "                                                                 \n",
      " patch_class_embedding (Patc  (None, 31, 64)           2048      \n",
      " hClassEmbedding)                                                \n",
      "                                                                 \n",
      " transformer_encoder (Transf  (None, 31, 64)           199936    \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 64)                0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               16640     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 20)                5140      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 227,156\n",
      "Trainable params: 227,156\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def build_act(transformer):\n",
    "        inputs = tf.keras.layers.Input(shape=(config['FRAMES'], \n",
    "                                              config[config['DATASET']]['KEYPOINTS']*config['CHANNELS']))\n",
    "        x = tf.keras.layers.Dense(d_model)(inputs)\n",
    "        x = PatchClassEmbedding(d_model, config['FRAMES'])(x)\n",
    "        x = transformer(x)\n",
    "        x = tf.keras.layers.Lambda(lambda x: x[:,0,:])(x)\n",
    "        x = tf.keras.layers.Dense(mlp_head_size)(x)\n",
    "        outputs = tf.keras.layers.Dense(config['CLASSES'])(x)\n",
    "        return tf.keras.models.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "transformer = TransformerEncoder(d_model, n_heads,d_ff, dropout, activation, n_layers)\n",
    "model = build_act(transformer)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#lr = CustomSchedule(d_model, \n",
    "#             warmup_steps=len(ds_train)*config['N_EPOCHS']*config['WARMUP_PERC'],\n",
    "#             decay_step=len(ds_train)*config['N_EPOCHS']*config['STEP_PERC'])\n",
    "\n",
    "optimizer = tfa.optimizers.AdamW(weight_decay=config['WEIGHT_DECAY'])\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "                           loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "                           metrics=[tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")])\n",
    "\n",
    "\n",
    "model.load_weights(\"bin/AcT_micro_3_9.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(tf.nn.softmax(model.predict(tf.concat(X, axis=0)), axis=-1),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    }
   ],
   "source": [
    "y_demo=model.predict(tf.expand_dims(x_train[0], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_demo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(tf.nn.softmax(y_demo,axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpose import MPOSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MPOSE2021 with openpose Pose Extractor\n",
      "Downloading Data...\n",
      "File exists in /home/ubuntu/.mpose/openpose.zip. specify overwrite=True if intended\n",
      "Extracting Data...\n",
      "File exists in /home/ubuntu/.mpose/openpose/. specify overwrite=True if intended\n"
     ]
    }
   ],
   "source": [
    "dataset = MPOSE(pose_extractor=\"openpose\", \n",
    "                split=split, \n",
    "                preprocess=None, \n",
    "                velocities=True, \n",
    "                remove_zip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=dataset.add_velocities(overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Dataset Information----\n",
      "Pose Extractor: openpose\n",
      "Split: 1\n",
      "X_train shape: (12562, 30, 25, 5)\n",
      "X_test shape: (2867, 30, 25, 5)\n",
      "Min-Max feature ranges:\n",
      "x: [0.0, 828.042]\n",
      "y: [0.0, 558.729]\n",
      "Vx: [-803.5, 790.405]\n",
      "Vy: [-511.387, 555.379]\n",
      "p: [0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "dataset.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.remove_velocities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_single = X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 25, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_single.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 17, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pre(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30, 52)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 205ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -4.5513544 ,   4.115604  ,   0.5044186 ,  -6.5964766 ,\n",
       "          3.8860729 , -17.330666  ,   3.5000796 ,   1.3987154 ,\n",
       "          6.0684814 ,   0.59741616,   4.355696  ,   2.273146  ,\n",
       "          2.7005196 ,   5.3428407 ,   0.8882222 ,   3.9699872 ,\n",
       "          3.7789156 ,  -5.280957  ,   1.6898061 ,   3.60288   ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_velocities(X_train,overwrite=False):\n",
    " \n",
    "        \n",
    "        seq_list = []\n",
    "        for seq in X_train:\n",
    "          \n",
    "            v1 = np.zeros((30+1, seq.shape[1], 3-1))\n",
    "            v2 = np.zeros((30+1, seq.shape[1], 3-1))\n",
    "            \n",
    "            v1[1:,...] = seq[:,:,:2]\n",
    "            v2[:30,...] = seq[:,:,:2]\n",
    "            vel = (v2-v1)[:-1,...]\n",
    "            data = np.concatenate((seq[:,:,:2], vel), axis=-1)\n",
    "            data = np.concatenate((data, seq[:,:,-1:]), axis=-1)       \n",
    "            seq_list.append(data)\n",
    "        X_train = np.stack(seq_list)\n",
    "\n",
    "        return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_velocities_single(seq,overwrite=False):\n",
    " \n",
    "        \n",
    "        seq_list = []\n",
    "  \n",
    "        \n",
    "        v1 = np.zeros((30+1, seq.shape[1], 3-1))\n",
    "        v2 = np.zeros((30+1, seq.shape[1], 3-1))\n",
    "        \n",
    "        v1[1:,...] = seq[:,:,:2]\n",
    "        v2[:30,...] = seq[:,:,:2]\n",
    "        vel = (v2-v1)[:-1,...]\n",
    "        data = np.concatenate((seq[:,:,:2], vel), axis=-1)\n",
    "        data = np.concatenate((data, seq[:,:,-1:]), axis=-1)       \n",
    "        seq_list.append(data)\n",
    "\n",
    "        return np.array(seq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = add_velocities_single(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30, 17, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = h = [0, 15, 16, 1, 14]\n",
    "right_foot = rf = [11, 12, 13,7]\n",
    "left_foot = lf = [8, 9, 10,4]\n",
    "prune= [1,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=X_train[0][:,0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 15, 5)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_keypoints_single(seq):\n",
    "    seq_list = []\n",
    "    to_prune = []\n",
    "    \n",
    "    for group in [h,rf, lf]:\n",
    "        if len(group) > 1:\n",
    "            to_prune.append(group[1:])\n",
    "    to_prune = [item for sublist in to_prune for item in sublist]\n",
    "    \n",
    "    print((to_prune))\n",
    "    #print(seq.shape)\n",
    "    seq[:,h[0],:] = np.true_divide(seq[:,h,:].sum(1), (seq[:,h,:] != 0).sum(1)+1e-9)\n",
    "    seq[:,rf[0],:] = np.true_divide(seq[:,rf,:].sum(1), (seq[:,rf,:] != 0).sum(1)+1e-9)\n",
    "    seq[:,lf[0],:] = np.true_divide(seq[:,lf,:].sum(1), (seq[:,lf,:] != 0).sum(1)+1e-9)\n",
    "    seq_list.append(seq)\n",
    "    np.array(seq_list)\n",
    "    X_train = np.delete(np.array(seq_list), to_prune, 2)\n",
    "\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 16, 1, 14, 12, 13, 7, 9, 10, 4]\n"
     ]
    }
   ],
   "source": [
    "x = reduce_keypoints_single(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30, 15, 5)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_keypoints(X_train):\n",
    "    seq_list = []\n",
    "    to_prune = []\n",
    "    \n",
    "    for group in [h,rf, lf]:\n",
    "        if len(group) > 1:\n",
    "            to_prune.append(group[1:])\n",
    "    to_prune = [item for sublist in to_prune for item in sublist]\n",
    "    \n",
    "    for seq in X_train:\n",
    "        #print(seq.shape)\n",
    "        seq[:,h[0],:] = np.true_divide(seq[:,h,:].sum(1), (seq[:,h,:] != 0).sum(1)+1e-9)\n",
    "        seq[:,rf[0],:] = np.true_divide(seq[:,rf,:].sum(1), (seq[:,rf,:] != 0).sum(1)+1e-9)\n",
    "        seq[:,lf[0],:] = np.true_divide(seq[:,lf,:].sum(1), (seq[:,lf,:] != 0).sum(1)+1e-9)\n",
    "        seq_list.append(seq)\n",
    "    X_train = np.stack(seq_list)\n",
    "    #X_train = np.delete(X_train, to_prune, 2)\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = reduce_keypoints(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30, 17, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_keypoints(X_train):\n",
    "    seq_list = []\n",
    "    to_prune = []\n",
    "    \n",
    "    for group in [h,rf, lf]:\n",
    "        if len(group) > 1:\n",
    "            to_prune.append(group[1:])\n",
    "    to_prune = [item for sublist in to_prune for item in sublist]\n",
    "    \n",
    "    for seq in X_train:\n",
    "        #print(seq.shape)\n",
    "        seq[:,h[0],:] = np.true_divide(seq[:,h,:].sum(1), (seq[:,h,:] != 0).sum(1)+1e-9)\n",
    "        seq[:,rf[0],:] = np.true_divide(seq[:,rf,:].sum(1), (seq[:,rf,:] != 0).sum(1)+1e-9)\n",
    "        seq[:,lf[0],:] = np.true_divide(seq[:,lf,:].sum(1), (seq[:,lf,:] != 0).sum(1)+1e-9)\n",
    "        seq_list.append(seq)\n",
    "    X_train = np.stack(seq_list)\n",
    "    X_train = np.delete(X_train, to_prune, 2)\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_1 = 1\n",
    "center_2 = 1\n",
    "module_keypoint_1 = 8\n",
    "module_keypoint_2 = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_center(X_train):    \n",
    "    for X in [X_train]:\n",
    "        seq_list = []\n",
    "        for seq in X:\n",
    "            pose_list = []\n",
    "            for pose in seq:\n",
    "                zero_point = (pose[center_1, :2] + pose[center_2,:2]) / 2\n",
    "                module_keypoint = (pose[module_keypoint_1, :2] + pose[module_keypoint_2,:2]) / 2\n",
    "                scale_mag = np.linalg.norm(zero_point - module_keypoint)\n",
    "                if scale_mag < 1:\n",
    "                    scale_mag = 1\n",
    "                pose[:,:2] = (pose[:,:2] - zero_point) / scale_mag\n",
    "                pose_list.append(pose)\n",
    "            seq = np.stack(pose_list)\n",
    "            seq_list.append(seq)\n",
    "        X = np.stack(seq_list)\n",
    "    \n",
    "    X_train = np.delete(X_train, prune, 2)\n",
    "    scaled = True\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = scale_and_center(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30, 13, 5)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_confidence(X_train):\n",
    "    X_train = X_train[...,:-1]\n",
    "    return X_train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_c = remove_confidence(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30, 13, 4)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_features(X_train):\n",
    "    X_train = X_train.reshape(X_train.shape[0], 30, -1)\n",
    "    return X_train\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_f = flatten_features(x_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30, 52)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=add_velocities(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = reduce_keypoints(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ = scale_and_center(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train  = remove_confidence(x_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = flatten_features(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12562, 30, 52)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 7s 7s/step\n"
     ]
    }
   ],
   "source": [
    "y=model.predict(x_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12562, 30, 25, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00],\n",
       "       [2.34870e+02, 6.64126e+01, 2.34870e+02, 6.64126e+01, 8.40771e-01],\n",
       "       [2.49527e+02, 6.71612e+01, 2.49527e+02, 6.71612e+01, 8.75739e-01],\n",
       "       [2.54645e+02, 1.00110e+02, 2.54645e+02, 1.00110e+02, 8.40024e-01],\n",
       "       [2.60504e+02, 1.25009e+02, 2.60504e+02, 1.25009e+02, 8.68385e-01],\n",
       "       [2.19502e+02, 6.49461e+01, 2.19502e+02, 6.49461e+01, 8.49073e-01],\n",
       "       [2.18773e+02, 9.57001e+01, 2.18773e+02, 9.57001e+01, 7.49432e-01],\n",
       "       [2.26101e+02, 1.09644e+02, 2.26101e+02, 1.09644e+02, 2.06299e-01],\n",
       "       [2.34149e+02, 1.19892e+02, 2.34149e+02, 1.19892e+02, 6.39379e-01],\n",
       "       [2.43674e+02, 1.20619e+02, 2.43674e+02, 1.20619e+02, 6.84198e-01],\n",
       "       [2.45144e+02, 1.60173e+02, 2.45144e+02, 1.60173e+02, 7.38917e-01],\n",
       "       [2.45136e+02, 2.00453e+02, 2.45136e+02, 2.00453e+02, 7.67821e-01],\n",
       "       [2.25329e+02, 1.19862e+02, 2.25329e+02, 1.19862e+02, 6.53879e-01],\n",
       "       [2.25361e+02, 1.55790e+02, 2.25361e+02, 1.55790e+02, 8.08271e-01],\n",
       "       [2.20954e+02, 1.90916e+02, 2.20954e+02, 1.90916e+02, 8.25551e-01],\n",
       "       [2.49514e+02, 4.29509e+01, 2.49514e+02, 4.29509e+01, 7.84337e-02],\n",
       "       [0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00],\n",
       "       [2.46597e+02, 4.37383e+01, 2.46597e+02, 4.37383e+01, 8.46595e-01],\n",
       "       [2.31951e+02, 4.37016e+01, 2.31951e+02, 4.37016e+01, 3.80911e-01],\n",
       "       [2.31972e+02, 1.90195e+02, 2.31972e+02, 1.90195e+02, 6.86278e-01],\n",
       "       [2.31209e+02, 1.89464e+02, 2.31209e+02, 1.89464e+02, 6.13726e-01],\n",
       "       [2.19474e+02, 1.96053e+02, 2.19474e+02, 1.96053e+02, 8.02246e-01],\n",
       "       [2.62695e+02, 2.01890e+02, 2.62695e+02, 2.01890e+02, 7.14654e-01],\n",
       "       [2.60492e+02, 2.03379e+02, 2.60492e+02, 2.03379e+02, 7.28853e-01],\n",
       "       [2.42908e+02, 2.04856e+02, 2.42908e+02, 2.04856e+02, 7.49616e-01]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aef76767ff8a5532355b7646b5cf041ccad72850b42f57d8c6c7a115b3b6c609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
